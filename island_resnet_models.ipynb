{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import csv\n",
    "from PIL import Image\n",
    "import glob\n",
    "import scipy\n",
    "from sklearn.cross_validation import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import tr_label\n",
    "tr_label_file = './data_island_in_the_sun/labels_training.csv'\n",
    "tr_label = open(tr_label_file, 'rt')\n",
    "tr_read = csv.reader(tr_label, delimiter=',', quoting=csv.QUOTE_NONE)\n",
    "tr_label_list = list(tr_read)\n",
    "y_data = np.array(tr_label_list)[1:].astype('int')\n",
    "#print(y_data)\n",
    "y_list = y_data[:,1:]\n",
    "\n",
    "num_data = y_list.shape[0]\n",
    "\n",
    "y_ = np.zeros([num_data,2])\n",
    "\n",
    "for i in range(num_data):\n",
    "    y_[i,y_list[i][0]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#import images\n",
    "filelist = [\"\" for x in range(1500)]\n",
    "\n",
    "for i in range(0, 1500):\n",
    "    path = \"./data_island_in_the_sun/train_data/\" + str(i) + \".tif\"\n",
    "    filelist[i] = path\n",
    "\n",
    "x_data = np.array([np.array(Image.open(fname)) for fname in filelist])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#prepocess \n",
    "x_list = x_data\n",
    "num_imgs = x_list.shape[0]\n",
    "\n",
    "down_imgs = []\n",
    "for i in range(num_imgs):\n",
    "    img = scipy.misc.imresize(x_list[i,:,:,:],[32,32,3])\n",
    "    down_imgs.append(img)\n",
    "    \n",
    "x_list = np.array(down_imgs)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(x_list, y_, test_size=0.2, random_state=0)\n",
    "# X_train.shape, y_train.shape\n",
    "# X_test.shape, y_test.shape\n",
    "\n",
    "num_train = X_train.shape[0]\n",
    "num_valid = X_test.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training Parameters\n",
    "learning_rate = 0.00001\n",
    "num_epochs = 300\n",
    "batch_size = 100\n",
    "display_step = 100\n",
    "img_width = 32\n",
    "\n",
    "\n",
    "\n",
    "# Network Parameters\n",
    "num_input = img_width*img_width*3 # data input (img shape: 101 * 101 * 3)\n",
    "num_classes = 2 # total classes (0/1)\n",
    "dropout = 0.75 # Dropout, probability to keep units\n",
    "\n",
    "BN_EPSILON = 0.001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# tf Graph input\n",
    "X = tf.placeholder(tf.float32, [None, img_width,img_width,3])\n",
    "Y = tf.placeholder(tf.float32, [None, num_classes])\n",
    "keep_prob = tf.placeholder(tf.float32) # dropout (keep probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_variables(name, shape, initializer=tf.contrib.layers.xavier_initializer(), is_fc_layer=False):\n",
    "    '''\n",
    "    :param name: A string. The name of the new variable\n",
    "    :param shape: A list of dimensions\n",
    "    :param initializer: User Xavier as default.\n",
    "    :param is_fc_layer: Want to create fc layer variable? May use different weight_decay for fc\n",
    "    layers.\n",
    "    :return: The created variable\n",
    "    '''\n",
    "    \n",
    "    ## TODO: to allow different weight decay to fully connected layer and conv layer\n",
    "    regularizer = tf.contrib.layers.l2_regularizer(scale=0.2)\n",
    "\n",
    "    new_variables = tf.get_variable(name, shape=shape, initializer=initializer,\n",
    "                                    regularizer=regularizer)\n",
    "    return new_variables\n",
    "\n",
    "\n",
    "def output_layer(input_layer, num_labels):\n",
    "    '''\n",
    "    :param input_layer: 2D tensor\n",
    "    :param num_labels: int. How many output labels in total? (10 for cifar10 and 100 for cifar100)\n",
    "    :return: output layer Y = WX + B\n",
    "    '''\n",
    "    input_dim = input_layer.get_shape().as_list()[-1]\n",
    "    fc_w = create_variables(name='fc_weights', shape=[input_dim, num_labels], is_fc_layer=True,\n",
    "                            initializer=tf.uniform_unit_scaling_initializer(factor=1.0))\n",
    "    fc_b = create_variables(name='fc_bias', shape=[num_labels], initializer=tf.zeros_initializer())\n",
    "\n",
    "    fc_h = tf.matmul(input_layer, fc_w) + fc_b\n",
    "    return fc_h\n",
    "\n",
    "\n",
    "def batch_normalization_layer(input_layer, dimension):\n",
    "    '''\n",
    "    Helper function to do batch normalziation\n",
    "    :param input_layer: 4D tensor\n",
    "    :param dimension: input_layer.get_shape().as_list()[-1]. The depth of the 4D tensor\n",
    "    :return: the 4D tensor after being normalized\n",
    "    '''\n",
    "    mean, variance = tf.nn.moments(input_layer, axes=[0, 1, 2])\n",
    "    beta = tf.get_variable('beta', dimension, tf.float32,\n",
    "                               initializer=tf.constant_initializer(0.0, tf.float32))\n",
    "    gamma = tf.get_variable('gamma', dimension, tf.float32,\n",
    "                                initializer=tf.constant_initializer(1.0, tf.float32))\n",
    "    bn_layer = tf.nn.batch_normalization(input_layer, mean, variance, beta, gamma, BN_EPSILON)\n",
    "\n",
    "    return bn_layer\n",
    "\n",
    "\n",
    "def conv_bn_relu_layer(input_layer, filter_shape, stride):\n",
    "    '''\n",
    "    A helper function to conv, batch normalize and relu the input tensor sequentially\n",
    "    :param input_layer: 4D tensor\n",
    "    :param filter_shape: list. [filter_height, filter_width, filter_depth, filter_number]\n",
    "    :param stride: stride size for conv\n",
    "    :return: 4D tensor. Y = Relu(batch_normalize(conv(X)))\n",
    "    '''\n",
    "\n",
    "    out_channel = filter_shape[-1]\n",
    "    filter = create_variables(name='conv', shape=filter_shape)\n",
    "\n",
    "    conv_layer = tf.nn.conv2d(input_layer, filter, strides=[1, stride, stride, 1], padding='SAME')\n",
    "    bn_layer = batch_normalization_layer(conv_layer, out_channel)\n",
    "\n",
    "    output = tf.nn.relu(bn_layer)\n",
    "    return output\n",
    "\n",
    "\n",
    "def bn_relu_conv_layer(input_layer, filter_shape, stride):\n",
    "    '''\n",
    "    A helper function to batch normalize, relu and conv the input layer sequentially\n",
    "    :param input_layer: 4D tensor\n",
    "    :param filter_shape: list. [filter_height, filter_width, filter_depth, filter_number]\n",
    "    :param stride: stride size for conv\n",
    "    :return: 4D tensor. Y = conv(Relu(batch_normalize(X)))\n",
    "    '''\n",
    "\n",
    "    in_channel = input_layer.get_shape().as_list()[-1]\n",
    "\n",
    "    bn_layer = batch_normalization_layer(input_layer, in_channel)\n",
    "    relu_layer = tf.nn.relu(bn_layer)\n",
    "\n",
    "    filter = create_variables(name='conv', shape=filter_shape)\n",
    "    conv_layer = tf.nn.conv2d(relu_layer, filter, strides=[1, stride, stride, 1], padding='SAME')\n",
    "    return conv_layer\n",
    "\n",
    "\n",
    "\n",
    "def residual_block(input_layer, output_channel, first_block=False):\n",
    "    '''\n",
    "    Defines a residual block in ResNet\n",
    "    :param input_layer: 4D tensor\n",
    "    :param output_channel: int. return_tensor.get_shape().as_list()[-1] = output_channel\n",
    "    :param first_block: if this is the first residual block of the whole network\n",
    "    :return: 4D tensor.\n",
    "    '''\n",
    "    input_channel = input_layer.get_shape().as_list()[-1]\n",
    "\n",
    "    # When it's time to \"shrink\" the image size, we use stride = 2\n",
    "    if input_channel * 2 == output_channel:\n",
    "        increase_dim = True\n",
    "        stride = 2\n",
    "    elif input_channel == output_channel:\n",
    "        increase_dim = False\n",
    "        stride = 1\n",
    "    else:\n",
    "        raise ValueError('Output and input channel does not match in residual blocks!!!')\n",
    "\n",
    "    # The first conv layer of the first residual block does not need to be normalized and relu-ed.\n",
    "    with tf.variable_scope('conv1_in_block'):\n",
    "        if first_block:\n",
    "            filter = create_variables(name='conv', shape=[3, 3, input_channel, output_channel])\n",
    "            conv1 = tf.nn.conv2d(input_layer, filter=filter, strides=[1, 1, 1, 1], padding='SAME')\n",
    "        else:\n",
    "            conv1 = bn_relu_conv_layer(input_layer, [3, 3, input_channel, output_channel], stride)\n",
    "\n",
    "    with tf.variable_scope('conv2_in_block'):\n",
    "        conv2 = bn_relu_conv_layer(conv1, [3, 3, output_channel, output_channel], 1)\n",
    "\n",
    "    # When the channels of input layer and conv2 does not match, we add zero pads to increase the\n",
    "    #  depth of input layers\n",
    "    if increase_dim is True:\n",
    "        pooled_input = tf.nn.avg_pool(input_layer, ksize=[1, 2, 2, 1],\n",
    "                                      strides=[1, 2, 2, 1], padding='VALID')\n",
    "        padded_input = tf.pad(pooled_input, [[0, 0], [0, 0], [0, 0], [input_channel // 2,\n",
    "                                                                     input_channel // 2]])\n",
    "    else:\n",
    "        padded_input = input_layer\n",
    "\n",
    "    output = conv2 + padded_input\n",
    "    return output\n",
    "\n",
    "\n",
    "def inference(input_tensor_batch, n, reuse):\n",
    "    '''\n",
    "    The main function that defines the ResNet. total layers = 1 + 2n + 2n + 2n +1 = 6n + 2\n",
    "    :param input_tensor_batch: 4D tensor\n",
    "    :param n: num_residual_blocks\n",
    "    :param reuse: To build train graph, reuse=False. To build validation graph and share weights\n",
    "    with train graph, resue=True\n",
    "    :return: last layer in the network. Not softmax-ed\n",
    "    '''\n",
    "\n",
    "    layers = []\n",
    "    with tf.variable_scope('conv0', reuse=reuse):\n",
    "        conv0 = conv_bn_relu_layer(input_tensor_batch, [3, 3, 3, 16], 1)\n",
    "        # activation_summary(conv0)\n",
    "        layers.append(conv0)\n",
    "\n",
    "    for i in range(n):\n",
    "        with tf.variable_scope('conv1_%d' %i, reuse=reuse):\n",
    "            if i == 0:\n",
    "                conv1 = residual_block(layers[-1], 16, first_block=True)\n",
    "            else:\n",
    "                conv1 = residual_block(layers[-1], 16)\n",
    "            # activation_summary(conv1)\n",
    "            layers.append(conv1)\n",
    "\n",
    "    for i in range(n):\n",
    "        with tf.variable_scope('conv2_%d' %i, reuse=reuse):\n",
    "            conv2 = residual_block(layers[-1], 32)\n",
    "            # activation_summary(conv2)\n",
    "            layers.append(conv2)\n",
    "\n",
    "    for i in range(n):\n",
    "        with tf.variable_scope('conv3_%d' %i, reuse=reuse):\n",
    "            conv3 = residual_block(layers[-1], 64)\n",
    "            layers.append(conv3)\n",
    "        assert conv3.get_shape().as_list()[1:] == [8, 8, 64]\n",
    "\n",
    "    with tf.variable_scope('fc', reuse=reuse):\n",
    "        in_channel = layers[-1].get_shape().as_list()[-1]\n",
    "        bn_layer = batch_normalization_layer(layers[-1], in_channel)\n",
    "        relu_layer = tf.nn.relu(bn_layer)\n",
    "        global_pool = tf.reduce_mean(relu_layer, [1, 2])\n",
    "\n",
    "        assert global_pool.get_shape().as_list()[-1:] == [64]\n",
    "        output = output_layer(global_pool, 2)\n",
    "        layers.append(output)\n",
    "\n",
    "    return layers[-1]\n",
    "\n",
    "\n",
    "# Create some wrappers for simplicity\n",
    "def conv2d(x, W, b, strides=1):\n",
    "    # Conv2D wrapper, with bias and relu activation\n",
    "    x = tf.nn.conv2d(x, W, strides=[1, strides, strides, 1], padding='SAME')\n",
    "    x = tf.nn.bias_add(x, b)\n",
    "    return tf.nn.relu(x)\n",
    "\n",
    "\n",
    "def maxpool2d(x, k=2):\n",
    "    # MaxPool2D wrapper\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, k, k, 1],\n",
    "                          padding='SAME')\n",
    "\n",
    "\n",
    "# Create model\n",
    "def conv_net(x, weights, biases, dropout):\n",
    "    # Tensor input become 4-D: [Batch Size, Height, Width, Channel]\n",
    "    \n",
    "    # x = tf.reshape(x, shape=[-1, img_width, img_width, 3])\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv1 = conv2d(x, weights['wc1'], biases['bc1'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv1 = maxpool2d(conv1, k=2)\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv2 = conv2d(conv1, weights['wc2'], biases['bc2'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    conv2 = maxpool2d(conv2, k=2)\n",
    "\n",
    "    # Fully connected layer\n",
    "    # Reshape conv2 output to fit fully connected layer input\n",
    "    fc1 = tf.contrib.layers.flatten(conv2)\n",
    "    # fc1 = tf.reshape(conv2, [-1, weights['wd1'].get_shape().as_list()[0]])\n",
    "    fc1 = tf.add(tf.matmul(fc1, weights['wd1']), biases['bd1'])\n",
    "    fc1 = tf.nn.relu(fc1)\n",
    "    # Apply Dropout\n",
    "    fc1 = tf.nn.dropout(fc1, dropout)\n",
    "\n",
    "    # Output, class prediction\n",
    "    out = tf.add(tf.matmul(fc1, weights['out']), biases['out'])\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Construct model\n",
    "# logits = conv_net(X, weights, biases, keep_prob)\n",
    "\n",
    "logits = inference(X, 2, reuse=False)\n",
    "prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss_op = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n",
    "    logits=logits, labels=Y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate)\n",
    "train_op = optimizer.minimize(loss_op)\n",
    "\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initialize the variables (i.e. assign their default value)\n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0, Minibatch Loss= 0.7490, Training Accuracy= 0.410\n",
      "('Testing Accuracy:', 0.38666666)\n",
      "Step 100, Minibatch Loss= 0.7055, Training Accuracy= 0.480\n",
      "('Testing Accuracy:', 0.46333331)\n",
      "Step 200, Minibatch Loss= 0.6139, Training Accuracy= 0.640\n",
      "('Testing Accuracy:', 0.57999998)\n",
      "Step 300, Minibatch Loss= 0.5413, Training Accuracy= 0.730\n",
      "('Testing Accuracy:', 0.71999997)\n",
      "Step 400, Minibatch Loss= 0.4509, Training Accuracy= 0.860\n",
      "('Testing Accuracy:', 0.78666663)\n",
      "Step 500, Minibatch Loss= 0.4192, Training Accuracy= 0.840\n",
      "('Testing Accuracy:', 0.8166666)\n",
      "Step 600, Minibatch Loss= 0.3502, Training Accuracy= 0.880\n",
      "('Testing Accuracy:', 0.83333331)\n",
      "Step 700, Minibatch Loss= 0.3116, Training Accuracy= 0.920\n",
      "('Testing Accuracy:', 0.8433333)\n",
      "Step 800, Minibatch Loss= 0.3368, Training Accuracy= 0.890\n",
      "('Testing Accuracy:', 0.84666663)\n",
      "Step 900, Minibatch Loss= 0.2508, Training Accuracy= 0.910\n",
      "('Testing Accuracy:', 0.84999996)\n",
      "Step 1000, Minibatch Loss= 0.2323, Training Accuracy= 0.940\n",
      "('Testing Accuracy:', 0.85333329)\n",
      "Step 1100, Minibatch Loss= 0.2668, Training Accuracy= 0.940\n",
      "('Testing Accuracy:', 0.84666663)\n",
      "Step 1200, Minibatch Loss= 0.1759, Training Accuracy= 0.960\n",
      "('Testing Accuracy:', 0.83999997)\n",
      "Step 1300, Minibatch Loss= 0.1737, Training Accuracy= 0.990\n",
      "('Testing Accuracy:', 0.83333331)\n",
      "Step 1400, Minibatch Loss= 0.1883, Training Accuracy= 0.950\n",
      "('Testing Accuracy:', 0.82666659)\n",
      "Step 1500, Minibatch Loss= 0.1192, Training Accuracy= 1.000\n",
      "('Testing Accuracy:', 0.82333326)\n",
      "Step 1600, Minibatch Loss= 0.1281, Training Accuracy= 0.990\n",
      "('Testing Accuracy:', 0.83333325)\n",
      "Step 1700, Minibatch Loss= 0.1252, Training Accuracy= 0.980\n",
      "('Testing Accuracy:', 0.81999993)\n",
      "Step 1800, Minibatch Loss= 0.0830, Training Accuracy= 1.000\n",
      "('Testing Accuracy:', 0.81999993)\n",
      "Step 1900, Minibatch Loss= 0.0930, Training Accuracy= 0.990\n",
      "('Testing Accuracy:', 0.81999993)\n",
      "Step 2000, Minibatch Loss= 0.0859, Training Accuracy= 0.990\n",
      "('Testing Accuracy:', 0.81333327)\n",
      "Step 2100, Minibatch Loss= 0.0625, Training Accuracy= 1.000\n",
      "('Testing Accuracy:', 0.81)\n",
      "Step 2200, Minibatch Loss= 0.0713, Training Accuracy= 1.000\n",
      "('Testing Accuracy:', 0.80666661)\n",
      "Step 2300, Minibatch Loss= 0.0603, Training Accuracy= 1.000\n",
      "('Testing Accuracy:', 0.80666667)\n",
      "Step 2400, Minibatch Loss= 0.0498, Training Accuracy= 1.000\n",
      "('Testing Accuracy:', 0.80999994)\n",
      "Step 2500, Minibatch Loss= 0.0575, Training Accuracy= 1.000\n",
      "('Testing Accuracy:', 0.80999994)\n",
      "Step 2600, Minibatch Loss= 0.0451, Training Accuracy= 1.000\n",
      "('Testing Accuracy:', 0.81)\n",
      "Step 2700, Minibatch Loss= 0.0408, Training Accuracy= 1.000\n",
      "('Testing Accuracy:', 0.81)\n",
      "Step 2800, Minibatch Loss= 0.0477, Training Accuracy= 1.000\n",
      "('Testing Accuracy:', 0.80999994)\n",
      "Step 2900, Minibatch Loss= 0.0358, Training Accuracy= 1.000\n",
      "('Testing Accuracy:', 0.81)\n",
      "Step 3000, Minibatch Loss= 0.0342, Training Accuracy= 1.000\n",
      "('Testing Accuracy:', 0.81000006)\n",
      "Step 3100, Minibatch Loss= 0.0403, Training Accuracy= 1.000\n",
      "('Testing Accuracy:', 0.80666661)\n",
      "Step 3200, Minibatch Loss= 0.0294, Training Accuracy= 1.000\n",
      "('Testing Accuracy:', 0.80666667)\n",
      "Step 3300, Minibatch Loss= 0.0290, Training Accuracy= 1.000\n",
      "('Testing Accuracy:', 0.79999995)\n",
      "Step 3400, Minibatch Loss= 0.0345, Training Accuracy= 1.000\n",
      "('Testing Accuracy:', 0.79999995)\n",
      "Step 3500, Minibatch Loss= 0.0247, Training Accuracy= 1.000\n",
      "('Testing Accuracy:', 0.80000001)\n",
      "Optimization Finished!\n",
      "('Testing Accuracy:', 0.79999995)\n"
     ]
    }
   ],
   "source": [
    "# Start training\n",
    "with tf.Session() as sess:\n",
    "\n",
    "    # Run the initializer\n",
    "    sess.run(init)\n",
    "\n",
    "    num_st = num_train/batch_size\n",
    "    \n",
    "    for i in range(num_epochs):\n",
    "        for step in range(num_st):\n",
    "            si,ei = step*batch_size,(step+1)*batch_size \n",
    "            batch_x, batch_y = X_train[si:ei ,:], y_train[si:ei,:]\n",
    "            # Run optimization op (backprop)\n",
    "            sess.run(train_op, feed_dict={X: batch_x, Y: batch_y, keep_prob: dropout})\n",
    "            if (i*num_st+step) % display_step == 0:\n",
    "                # Calculate batch loss and accuracy\n",
    "                loss, acc = sess.run([loss_op, accuracy], feed_dict={X: batch_x,\n",
    "                                                                     Y: batch_y,\n",
    "                                                                     keep_prob: 1.0})\n",
    "                print(\"Step \" + str(i*num_st+step) + \", Minibatch Loss= \" + \\\n",
    "                      \"{:.4f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                      \"{:.3f}\".format(acc))\n",
    "\n",
    "                print(\"Testing Accuracy:\", \\\n",
    "                    sess.run(accuracy, feed_dict={X: X_test,\n",
    "                                                  Y: y_test,\n",
    "                                                  keep_prob: 1.0}))                \n",
    "\n",
    "    print(\"Optimization Finished!\")\n",
    "\n",
    "    # Calculate accuracy for 256 MNIST test images\n",
    "    print(\"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={X: X_test,\n",
    "                                      Y: y_test,\n",
    "                                      keep_prob: 1.0}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 32, 32, 3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 2)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python2",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
